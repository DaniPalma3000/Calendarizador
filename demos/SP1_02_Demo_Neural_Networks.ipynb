{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaniPalma3000/Calendarizador/blob/master/demos/SP1_02_Demo_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEMO #02\n",
        "\n",
        "What you will learn:\n",
        "\n",
        "\n",
        "*   How to manipulate Datasets with Pandas\n",
        "*   What is Tensorflow and how to use it\n",
        "*   How to create Neural Networks for Regression and Classification Tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "_LXEnfcKV8AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas (Dataset Manipulation)\n",
        "\n",
        "\"*pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.*\"\n",
        "\n",
        "[Panda's Official Documentation](https://pandas.pydata.org)\n",
        "\n",
        "-----\n",
        "\n",
        "Pandas is a powerful library used to manipulate and analyze data. This library imitates R's philosophy and Sintax (both are very similar!)\n",
        "\n",
        "Pandas is powerful at **manipulating structured data**, so keep in mind this library will shine with table-looking shaped datasets.\n",
        "In few lines, you can do a lot of stuff!\n",
        "\n",
        "Advantages:\n",
        "* Provides efficient and optimized ways to manipulate data\n",
        "* Supports multiple file formats\n",
        "* Can easily merge, divide, join, filter and other type of operations in a similar fashion to SQL!\n",
        "\n",
        "Limitaitons:\n",
        "* Can only deal with structured data\n",
        "\n",
        "Recommended Resources:\n",
        "* [Official Documentation](https://pandas.pydata.org/docs/)\n",
        "* [Geeksforgeeks](https://www.geeksforgeeks.org/pandas-tutorial/)\n",
        "* [w3Schools](https://www.w3schools.com/python/pandas/default.asp)\n"
      ],
      "metadata": {
        "id": "_ehfIR--WP8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset"
      ],
      "metadata": {
        "id": "VweteaNsr_rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "All the information regarding the dataset used for this demo can be found in the following link:\n",
        "https://archive.ics.uci.edu/ml/datasets/Computer+Hardware\n",
        "'''\n",
        "\n",
        "# Getting Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data"
      ],
      "metadata": {
        "id": "Mmt-KsURr09V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4a4663-2fb9-4baa-9a39-3e509cc047e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-06 00:40:26--  https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘machine.data’\n",
            "\n",
            "machine.data            [ <=>                ]   8.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-06 00:40:26 (110 MB/s) - ‘machine.data’ saved [8726]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For dataset manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Loading Dataset and have a glimpse about it\n",
        "column_names = ['Vendor','Model','MYCT','MMIN','MMAX',\n",
        "                'CACH', 'CHMIN', 'CHMAX', 'PRP', 'ERP']\n",
        "\n",
        "raw_dataset = pd.read_csv(\"machine.data\", names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True)"
      ],
      "metadata": {
        "id": "sv3boXiQsDes"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .describe()"
      ],
      "metadata": {
        "id": "XSJVNnNgsGDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "metadata": {
        "id": "K32sABzOsPZU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "77bfae2f-8b3c-4f78-a52d-1b3eafd1059d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              MYCT          MMIN          MMAX        CACH       CHMIN  \\\n",
              "count   209.000000    209.000000    209.000000  209.000000  209.000000   \n",
              "mean    203.822967   2867.980861  11796.153110   25.205742    4.698565   \n",
              "std     260.262926   3878.742758  11726.564377   40.628722    6.816274   \n",
              "min      17.000000     64.000000     64.000000    0.000000    0.000000   \n",
              "25%      50.000000    768.000000   4000.000000    0.000000    1.000000   \n",
              "50%     110.000000   2000.000000   8000.000000    8.000000    2.000000   \n",
              "75%     225.000000   4000.000000  16000.000000   32.000000    6.000000   \n",
              "max    1500.000000  32000.000000  64000.000000  256.000000   52.000000   \n",
              "\n",
              "            CHMAX          PRP          ERP  \n",
              "count  209.000000   209.000000   209.000000  \n",
              "mean    18.267943   105.622010    99.330144  \n",
              "std     25.997318   160.830733   154.757102  \n",
              "min      0.000000     6.000000    15.000000  \n",
              "25%      5.000000    27.000000    28.000000  \n",
              "50%      8.000000    50.000000    45.000000  \n",
              "75%     24.000000   113.000000   101.000000  \n",
              "max    176.000000  1150.000000  1238.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96c0590e-4794-4a27-b91f-eecf10f30a93\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MYCT</th>\n",
              "      <th>MMIN</th>\n",
              "      <th>MMAX</th>\n",
              "      <th>CACH</th>\n",
              "      <th>CHMIN</th>\n",
              "      <th>CHMAX</th>\n",
              "      <th>PRP</th>\n",
              "      <th>ERP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>203.822967</td>\n",
              "      <td>2867.980861</td>\n",
              "      <td>11796.153110</td>\n",
              "      <td>25.205742</td>\n",
              "      <td>4.698565</td>\n",
              "      <td>18.267943</td>\n",
              "      <td>105.622010</td>\n",
              "      <td>99.330144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>260.262926</td>\n",
              "      <td>3878.742758</td>\n",
              "      <td>11726.564377</td>\n",
              "      <td>40.628722</td>\n",
              "      <td>6.816274</td>\n",
              "      <td>25.997318</td>\n",
              "      <td>160.830733</td>\n",
              "      <td>154.757102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>50.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>28.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>110.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>8000.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>45.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>225.000000</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>16000.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>113.000000</td>\n",
              "      <td>101.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1500.000000</td>\n",
              "      <td>32000.000000</td>\n",
              "      <td>64000.000000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>1150.000000</td>\n",
              "      <td>1238.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96c0590e-4794-4a27-b91f-eecf10f30a93')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96c0590e-4794-4a27-b91f-eecf10f30a93 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96c0590e-4794-4a27-b91f-eecf10f30a93');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"raw_dataset\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"MYCT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 483.97050871018877,\n        \"min\": 17.0,\n        \"max\": 1500.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          203.82296650717703,\n          110.0,\n          209.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MMIN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10729.41954443447,\n        \"min\": 64.0,\n        \"max\": 32000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2867.9808612440193,\n          2000.0,\n          209.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MMAX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20814.240639318406,\n        \"min\": 64.0,\n        \"max\": 64000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          11796.153110047846,\n          8000.0,\n          209.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CACH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 101.31952594216281,\n        \"min\": 0.0,\n        \"max\": 256.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          209.0,\n          25.205741626794257,\n          32.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CHMIN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 72.29721648974446,\n        \"min\": 0.0,\n        \"max\": 209.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          4.698564593301436,\n          2.0,\n          209.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CHMAX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 83.79922144431414,\n        \"min\": 0.0,\n        \"max\": 209.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          18.267942583732058,\n          8.0,\n          209.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PRP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 378.8188922529687,\n        \"min\": 6.0,\n        \"max\": 1150.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          105.622009569378,\n          50.0,\n          209.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ERP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 410.0213432663018,\n        \"min\": 15.0,\n        \"max\": 1238.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          99.33014354066985,\n          45.0,\n          209.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .head() and .tail()"
      ],
      "metadata": {
        "id": "AsADBMgXsNHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the dataset (takes the top N rows)\n",
        "raw_dataset.head(n=10)"
      ],
      "metadata": {
        "id": "0KKpcKkUsirm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the dataset (takes the bottom N rows)\n",
        "raw_dataset.tail(n=5)"
      ],
      "metadata": {
        "id": "iuNRuBBxssmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shape"
      ],
      "metadata": {
        "id": "XFX2Y26B1akG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape is a fancy way of calling Dataset's dimension\n",
        "raw_dataset.shape"
      ],
      "metadata": {
        "id": "xwqNcqop1bjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column Manipulation"
      ],
      "metadata": {
        "id": "sW0AAh8xs8Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Column"
      ],
      "metadata": {
        "id": "NiBaGrAgtdkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Like a dictionary, pass the EXACT name of the column to extract one column\n",
        "raw_dataset[\"Vendor\"]"
      ],
      "metadata": {
        "id": "DQg_GO4ytV_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add column\n",
        "\n"
      ],
      "metadata": {
        "id": "wvyLvhHTylm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To extract multiple, use an array instead\n",
        "raw_dataset[\"One\"] = 1\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "xTo6Bgetu7uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete Column\n"
      ],
      "metadata": {
        "id": "pJuv1IBszUEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset.pop(\"One\")\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "dFrAEXs1zV6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Column Values"
      ],
      "metadata": {
        "id": "mzqeXJmuvxgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all the unique values\n",
        "raw_dataset[\"Vendor\"].unique()"
      ],
      "metadata": {
        "id": "JAHu_eP1v27m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the values of the column\n",
        "raw_dataset[\"Vendor\"].value_counts()"
      ],
      "metadata": {
        "id": "t0YA5BglwHCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Row Manipulation"
      ],
      "metadata": {
        "id": "QOElaoWcwPGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"SELECT\" and \"WHERE\""
      ],
      "metadata": {
        "id": "lascQMhSwRjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows that meet a column value criteria\n",
        "raw_dataset.loc[raw_dataset[\"Vendor\"] == \"ibm\"]"
      ],
      "metadata": {
        "id": "9Rgk_ZhdwwOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows that meet a column value criteria\n",
        "raw_dataset.loc[raw_dataset[\"MYCT\"] == 25]"
      ],
      "metadata": {
        "id": "ZzMX5FFUxJZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine conditions\n",
        "raw_dataset.loc[(raw_dataset[\"MYCT\"] <= 25) & (raw_dataset[\"MYCT\"] >= 1)]"
      ],
      "metadata": {
        "id": "hQkqpza3xbAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine conditions\n",
        "raw_dataset.loc[(raw_dataset[\"Vendor\"] == \"ibm\") & (raw_dataset[\"MYCT\"] <= 25)]"
      ],
      "metadata": {
        "id": "yI8RkTSVx7f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Subset"
      ],
      "metadata": {
        "id": "NEsdOKFfz29G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffles the rows and then returns the fraction desired\n",
        "raw_dataset.sample(frac = 0.5, random_state = 1000)"
      ],
      "metadata": {
        "id": "9o2iNrqez6mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge"
      ],
      "metadata": {
        "id": "WZSmCTn50X1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets make two dataset dummies\n",
        "dataset_one = raw_dataset.sample(frac = 0.5, random_state = 100)\n",
        "dataset_two = raw_dataset.sample(frac = 0.5, random_state = 250)"
      ],
      "metadata": {
        "id": "nVBzJvdL0Zq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No Information Loss Merge\n",
        "pd.concat([dataset_one, dataset_two])"
      ],
      "metadata": {
        "id": "UNklq7yV1Fzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JOIN merge\n",
        "pd.concat([dataset_one, dataset_two], join=\"inner\")"
      ],
      "metadata": {
        "id": "pC3BH4Lu1luE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Practices\n",
        "\n",
        "* Before loading a dataset, check the README or any file that contains information about it\n",
        "* When you are modifying a dataset, don't reuse the same variable → create a new one\n",
        "* Use descriptive names for variables → will save you a lot of hassle"
      ],
      "metadata": {
        "id": "Qh0T6-s-kl1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity\n",
        "\n",
        "* Download the [Balance Scale Dataset](https://archive.ics.uci.edu/ml/datasets/Balance+Scale)\n",
        "* Do a preview with by using .describe() and either head() or tail()\n",
        "* Check its shape\n",
        "* Remove the column \"Class Name\"\n",
        "* Create two random Subsets of the modified dataset (both with a fraction of 65%)\n",
        "* Perform an Inner Join and check the resulting shape\n"
      ],
      "metadata": {
        "id": "4m_81xTKlrcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "VxM5CwKxluFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Tensorflow](https://www.tensorflow.org/?hl=es-419) (AI library)\n",
        "\n",
        "TL;DR: A platform and library to work on AI related projects. It is user friendly and is easy to learn.\n",
        "\n",
        "Advantages:\n",
        "* Allows to train any kind of AI: from the simplest one to the craziest ones\n",
        "* It has Web, PC and edge technology dispositives (such as phones and microcontorllers) coverage\n",
        "* Widely used and well documented\n",
        "* Available for Python and JavaScript\n",
        "* Posseses great abstraction capabilities → User only cares about creating and technical details are hidden from them\n",
        "\n",
        "Tensorflow works with the philosophy of \"tensors flowing\" or vectors moving from one point to another.\n",
        "The library helps to develop dataflow graphs that describe how data traverses through a multidimensional graph, composed by nodes.\n",
        "\n"
      ],
      "metadata": {
        "id": "uQU0Vn6aYV3o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2hOe4K0Hmut"
      },
      "source": [
        "# 0) Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWUjG0IIJkk"
      },
      "source": [
        "# ----- Libraries ----- #\n",
        "\n",
        "# This is the main Library that allows us to work with Neural Networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# For graph plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.math import confusion_matrix\n",
        "\n",
        "# For dataset manipulation\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# For visualizing more complex graphs\n",
        "import seaborn as sns\n",
        "\n",
        "# Global constant for training acceleration\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE_cmElZJ4PE"
      },
      "source": [
        "# Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARASHphaKU6D"
      },
      "source": [
        "## 1) Dataset Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KMmvqGMGOXu"
      },
      "source": [
        "'''\n",
        "All the information regarding the dataset used for this demo can be found in the following link:\n",
        "https://archive.ics.uci.edu/ml/datasets/Computer+Hardware\n",
        "'''\n",
        "\n",
        "# Getting Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgXcpexoIG9P",
        "collapsed": true
      },
      "source": [
        "# Loading Dataset and have a glimpse about it\n",
        "column_names = ['Vendor','Model','MYCT','MMIN','MMAX',\n",
        "                'CACH', 'CHMIN', 'CHMAX', 'PRP', 'ERP']\n",
        "\n",
        "raw_dataset = pd.read_csv(\"machine.data\", names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True)\n",
        "\n",
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W3AsBCPQMHu"
      },
      "source": [
        "# Lets check columns\n",
        "raw_dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsYYb14mKiz6"
      },
      "source": [
        "# Summary of the dataset\n",
        "raw_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc8kp61PKzl5"
      },
      "source": [
        "# Returns a form of (# rows, # columns)\n",
        "raw_dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taL7hSd9LHpj"
      },
      "source": [
        "# Lets make a copy\n",
        "new_dataset = raw_dataset.copy()\n",
        "\n",
        "# Lets check for null values\n",
        "print(new_dataset.isna().sum())\n",
        "\n",
        "# Dropping null rows\n",
        "new_dataset = new_dataset.dropna()\n",
        "\n",
        "# Checking new dataset\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-wSRqvEPoLo"
      },
      "source": [
        "# Lets visualize the data\n",
        "sns.pairplot(new_dataset[['MYCT','MMIN','MMAX',\n",
        "                'CACH', 'CHMIN', 'CHMAX', 'PRP']], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKsREMcO91a"
      },
      "source": [
        "## 2) NN for a simple Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xolCD5p4QFa8"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = ['MYCT','MMIN','MMAX','CACH', 'CHMIN', 'CHMAX', 'PRP']\n",
        "dependent_variables = ['ERP']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "id": "XqadtySjOXtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target"
      ],
      "metadata": {
        "id": "h1eEAOkqOan-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efOETJ7cPkMO"
      },
      "source": [
        "# Lets build a simple model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "\n",
        "  # Last layer of the model and its activation function decide if it is a regression or classification problem!\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='relu'),\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(\n",
        "    loss='mse',\n",
        "    optimizer='adam',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG1tt040L5d0"
      },
      "source": [
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=15,\n",
        "          batch_size = 32,\n",
        "          validation_split=0.2,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBGnFIGtJ_9e"
      },
      "source": [
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l31mWppGPKRf"
      },
      "source": [
        "## 3) NN for multiple Regressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2faCY4iaPPgl"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = ['MYCT','MMIN','MMAX','CACH', 'CHMIN', 'CHMAX']\n",
        "dependent_variables = ['PRP', 'ERP']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRBSUiPqPWDl"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=len(dependent_variables), activation='relu'),\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=10,\n",
        "          batch_size = 32 ,\n",
        "          validation_split=0.2\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "metadata": {
        "id": "9zFFNUGwcnLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny4qW8UlP5LQ"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okEGIBRLLg1z"
      },
      "source": [
        "## 1) Dataset Preparations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "All the information regarding the dataset used for this demo can be found in the following link:\n",
        "https://archive.ics.uci.edu/ml/datasets/Iris\n",
        "'''\n",
        "\n",
        "# Getting Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
      ],
      "metadata": {
        "id": "bG6-fqP_dg7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncn_CmxWYqL1"
      },
      "source": [
        "# Loading Dataset and have a glimpse about it\n",
        "column_names = ['sepal_length','sepal_width','petal_length','petal_width', 'class']\n",
        "\n",
        "raw_dataset = pd.read_csv(\"iris.data\", names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\",\", skipinitialspace=True)\n",
        "\n",
        "# Brief Statistical Summary of the dataset\n",
        "raw_dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I53Qd8obhFg"
      },
      "source": [
        "# Lets check columns\n",
        "raw_dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2yDVbIpblVZ"
      },
      "source": [
        "# Summary of the dataset\n",
        "raw_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8y5U8Tbnqf"
      },
      "source": [
        "# Returns a form of (# rows, # columns)\n",
        "raw_dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19wGecl7bpJd"
      },
      "source": [
        "# Lets make a copy\n",
        "new_dataset = raw_dataset.copy()\n",
        "\n",
        "# Lets check for null values\n",
        "# df.dropna()\n",
        "print(new_dataset.isna().sum())\n",
        "\n",
        "# Dropping null rows\n",
        "new_dataset = new_dataset.dropna()\n",
        "\n",
        "# Checking new dataset\n",
        "new_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SwagrZYbpp2"
      },
      "source": [
        "# Lets visualize the data\n",
        "sns.pairplot(new_dataset[['sepal_length','sepal_width','petal_length','petal_width']], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QchVy_bq_t"
      },
      "source": [
        "# Splitting dataset into training and testing\n",
        "train, test = train_test_split(new_dataset, test_size=0.2)\n",
        "\n",
        "# Sepparating both sets into dependent and independent variables\n",
        "independent_variables = list(raw_dataset.columns)\n",
        "independent_variables.remove('class')\n",
        "dependent_variables = ['class']\n",
        "\n",
        "train_set = train[independent_variables]\n",
        "train_target = train[dependent_variables]\n",
        "\n",
        "test_set = test[independent_variables]\n",
        "test_target = test[dependent_variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target, test_target"
      ],
      "metadata": {
        "id": "0ZbOA7bGfrNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NNs can't understand strings, we have to ENCODE them! (more next class)\n",
        "train_target = pd.factorize(train_target['class'])[0]\n",
        "test_target = pd.factorize(test_target['class'])[0]"
      ],
      "metadata": {
        "id": "Piu9aiz9fp0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target, test_target"
      ],
      "metadata": {
        "id": "fLsO0AOnKYzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9NmOn_vecR3"
      },
      "source": [
        "## 2) NN for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17v9Qk4xcIPi"
      },
      "source": [
        "# Lets build the model. NOTE: this is the construction of the architecture of the model!\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(len(independent_variables))),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "  tf.keras.layers.Dense(units=3, activation='softmax')\n",
        "  ])\n",
        "\n",
        "# Now lets compile the model. NOTE: These are the finishing touches before having a fully functional model\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Now lets train the model!\n",
        "model.fit(train_set,\n",
        "          train_target,\n",
        "          epochs=25,\n",
        "          batch_size = 128,\n",
        "          validation_split=0.2\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets evaluate our model\n",
        "model.evaluate(x=test_set, y=test_target, batch_size=128)"
      ],
      "metadata": {
        "id": "gTH5pRN2e_lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_target"
      ],
      "metadata": {
        "id": "uu0j1s6BMUkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aTmhnXieaFe"
      },
      "source": [
        "# Function that plots confusion matrix\n",
        "def plot_confusion_matrix(labels, predictions):\n",
        "  figure = plt.figure(figsize=(4, 4))\n",
        "  sns.heatmap(confusion_matrix(labels=labels, predictions=predictions), annot=True,cmap=plt.cm.Blues)\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "predictions = list(map(lambda x: np.argmax(x), model.predict(test_set)))\n",
        "\n",
        "plot_confusion_matrix(labels=test_target, predictions=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s3DicqEjZAu"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO7kBZ-MPLaP"
      },
      "source": [
        "## 1) Data Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAXYOI9RZxxM"
      },
      "source": [
        "import itertools\n",
        "from keras.preprocessing import image\n",
        "\n",
        "IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 256\n",
        "\n",
        "img_rows = [(i-(IMG_WIDTH/2))/(IMG_WIDTH/2) for i in range(IMG_WIDTH)]\n",
        "img_cols = [(j-(IMG_HEIGHT/2))/(IMG_HEIGHT/2) for j in range(IMG_HEIGHT)]\n",
        "\n",
        "flatten_image = np.array(list(itertools.product(img_rows, img_cols)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht0MNfvfr5cQ"
      },
      "source": [
        "## 2) NN for Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ7hMmVEr8LU"
      },
      "source": [
        "# Creating a custom Layer\n",
        "class ScaleLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, redScale=255.0, greenScale=255.0, blueScale=255.0):\n",
        "    super(ScaleLayer, self).__init__()\n",
        "    self.scale = tf.constant([redScale, greenScale, blueScale], dtype=tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = tf.dtypes.cast(inputs, tf.float32)\n",
        "    outputs = outputs * self.scale\n",
        "    return tf.dtypes.cast(outputs, tf.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2BrfvIysFzD"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(2, )),\n",
        "  tf.keras.layers.Dense(units=128, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=256, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=512, activation='tanh', kernel_constraint=tf.keras.constraints.MaxNorm(max_value=4), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4)),\n",
        "  tf.keras.layers.Dense(units=3, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal()),\n",
        "  ScaleLayer(redScale=0, greenScale=255, blueScale=0)\n",
        "  ])\n",
        "\n",
        "generated_image = np.reshape(np.array(model(flatten_image)), newshape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image.array_to_img(generated_image))\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}